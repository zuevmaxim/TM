{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T07:56:55.014916Z",
     "start_time": "2023-10-24T07:56:54.401019Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7c00f5cbaa5858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T07:56:56.564169Z",
     "start_time": "2023-10-24T07:56:55.015703Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                              title  \\\n",
       "0  1987  Self-Organization of Associative Database and ...   \n",
       "1  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
       "2  1988  Storing Covariance by the Associative Long-Ter...   \n",
       "3  1994  Bayesian Query Construction for Neural Network...   \n",
       "4  1994  Neural Network Ensembles, Cross Validation, an...   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.read_csv('data/nips/papers.csv')\n",
    "papers = papers.drop(columns=['id', 'event_type', 'pdf_name', 'abstract'], axis=1)\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3582f59b8c2d9a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T07:56:57.542458Z",
     "start_time": "2023-10-24T07:56:56.563699Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Maksim.Zuev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'[\\d_]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = set(map(preprocess_text, stop_words))\n",
    "    \n",
    "    \n",
    "def text_to_words(text: str) -> List[str]:\n",
    "    return text.split(\" \")\n",
    "\n",
    "def filter_stop_words(text: List[str]) -> List[str]:\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "import nltk\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def stem_words(text: List[str]) -> List[str]:\n",
    "    return [sno.stem(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b730bf92766e9a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T07:57:24.728131Z",
     "start_time": "2023-10-24T07:56:57.537298Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selforganization, of, associative, database, and, its, applications, hisashi, suzuki, and, ...',\n",
       " 'a, mean, field, theory, of, layer, iv, of, visual, cortex, ...',\n",
       " 'storing, covariance, by, the, associative, longterm, potentiation, and, depression, of, ...',\n",
       " 'bayesian, query, construction, for, neural, network, models, gerhard, paass, jorg, ...',\n",
       " 'neural, network, ensembles, cross, validation, and, active, learning, anders, krogh, ...']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_texts = papers['paper_text'].map(preprocess_text).to_list()\n",
    "preprocessed_documents = [text_to_words(document) for document in preprocessed_texts]\n",
    "list(map(lambda t: \", \".join(t[:10]) + \", ...\", preprocessed_documents[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a746379fecaf2841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T07:57:28.463170Z",
     "start_time": "2023-10-24T07:57:24.943919Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selforganization, associative, database, applications, hisashi, suzuki, suguru, arimoto, osaka, university, ...',\n",
       " 'mean, field, theory, layer, iv, visual, cortex, application, artificial, neural, ...',\n",
       " 'storing, covariance, associative, longterm, potentiation, depression, synaptic, strengths, hippocampus, patric, ...',\n",
       " 'bayesian, query, construction, neural, network, models, gerhard, paass, jorg, kindermann, ...',\n",
       " 'neural, network, ensembles, cross, validation, active, learning, anders, krogh, nordita, ...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_documents = [filter_stop_words(document) for document in preprocessed_documents]\n",
    "list(map(lambda t: \", \".join(t[:10]) + \", ...\", filtered_documents[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ec4b1331ae27d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T07:57:29.214930Z",
     "start_time": "2023-10-24T07:57:28.457611Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# all_words = ' '.join([' '.join(text) for text in filtered_documents])\n",
    "# wordcloud = WordCloud(background_color=\"white\", max_words=100, contour_width=3, contour_color='steelblue')\n",
    "# wordcloud.generate(all_words)\n",
    "# wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a5d1f125a3a215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:01:12.876902Z",
     "start_time": "2023-10-24T07:57:29.390341Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selforgan, associ, databas, applic, hisashi, suzuki, suguru, arimoto, osaka, univers, ...',\n",
       " 'mean, field, theori, layer, iv, visual, cortex, applic, artifici, neural, ...',\n",
       " 'store, covari, associ, longterm, potenti, depress, synapt, strength, hippocampus, patric, ...',\n",
       " 'bayesian, queri, construct, neural, network, model, gerhard, paass, jorg, kindermann, ...',\n",
       " 'neural, network, ensembl, cross, valid, activ, learn, ander, krogh, nordita, ...']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_documents = [stem_words(document) for document in filtered_documents]\n",
    "list(map(lambda t: \", \".join(t[:10]) + \", ...\", stemmed_documents[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fce9e9b86a3e31f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T19:22:30.779919Z",
     "start_time": "2023-10-25T19:22:30.774896Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "documents = stemmed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a44a209ea218ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T19:22:47.732746Z",
     "start_time": "2023-10-25T19:22:32.994331Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "id2word = corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbcb06250dc6153c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T19:22:47.740461Z",
     "start_time": "2023-10-25T19:22:47.734834Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count = 249310\n",
      "documents count = 7241\n",
      "topics count = 20\n"
     ]
    }
   ],
   "source": [
    "word_count = len(id2word.cfs)\n",
    "print(f\"word count = {word_count}\")\n",
    "documents_count = id2word.num_docs\n",
    "print(f\"documents count = {documents_count}\")\n",
    "topics_count = 20\n",
    "print(f\"topics count = {topics_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47fbe6544361b3ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:01:15.066609Z",
     "start_time": "2023-10-24T08:01:14.438697Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abolish -> 1, abstract -> 1, acceler -> 1, accept -> 2, accomplish -> 1, accord -> 6, achiev -> 2, actual -> 3, adap -> 1, add -> 2, address -> 2, adjac -> 1, admitt -> 1, adopt -> 1, advanc -> 2, al -> 4, algorithm -> 8, almighti -> 1, alreadi -> 1, also -> 2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_in_doc = [id2word.doc2bow(doc) for doc in documents]\n",
    "\", \".join(map(lambda t: f\"{id2word[t[0]]} -> {t[1]}\", word_count_in_doc[0][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c31cef6ac5028531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:45:33.858481Z",
     "start_time": "2023-10-24T08:01:15.064934Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "phi = np.random.rand(word_count, topics_count)\n",
    "theta = np.random.rand(topics_count, documents_count)\n",
    "n_word_topic = np.zeros((word_count, topics_count))\n",
    "n_topic_document = np.zeros((topics_count, documents_count))\n",
    "\n",
    "for _ in range(100):\n",
    "    n_word_topic.fill(0)\n",
    "    n_topic_document.fill(0)\n",
    "    for d in range(documents_count):\n",
    "        for (w, wc) in word_count_in_doc[d]:\n",
    "            z = np.dot(phi[w, :], theta[:, d])\n",
    "            for t in range(topics_count):\n",
    "                k = phi[w, t] * theta[t, d]\n",
    "                if k > 0:\n",
    "                    delta = wc * k / z\n",
    "                    n_word_topic[w, t] += delta\n",
    "                    n_topic_document[t, d] += delta\n",
    "    for t in range(topics_count):\n",
    "        n_t = np.sum(n_word_topic[:, t])\n",
    "        phi[:, t] += n_word_topic[:, t] / n_t\n",
    "    for d in range(documents_count):\n",
    "        n_d = np.sum(n_topic_document[:, d])\n",
    "        theta[:, d] += n_topic_document[:, d] / n_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d2bbb2ceea8c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T20:21:36.797536Z",
     "start_time": "2023-10-24T20:21:36.651483Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for topic 0: model, use, data, algorithm, time, system, e, function, one, figur\n",
      "Top words for topic 1: use, x, function, j, input, process, problem, differ, k, system\n",
      "Top words for topic 2: model, network, use, algorithm, output, n, case, problem, condit, class\n",
      "Top words for topic 3: use, e, input, network, c, function, p, valu, comput, one\n",
      "Top words for topic 4: learn, use, k, estim, l, figur, inform, problem, b, number\n",
      "Top words for topic 5: network, use, learn, function, algorithm, input, perform, p, system, model\n",
      "Top words for topic 6: use, n, j, e, figur, data, neural, state, vector, one\n",
      "Top words for topic 7: use, comput, unit, algorithm, neural, valu, n, learn, process, experi\n",
      "Top words for topic 8: set, model, n, data, problem, f, result, paramet, algorithm, x\n",
      "Top words for topic 9: learn, train, input, data, model, j, use, valu, r, differ\n",
      "Top words for topic 10: use, x, train, result, two, method, valu, one, function, problem\n",
      "Top words for topic 11: x, input, set, result, data, w, learn, k, time, figur\n",
      "Top words for topic 12: network, use, r, algorithm, neural, one, figur, x, perform, valu\n",
      "Top words for topic 13: e, model, x, b, distribut, input, v, error, weight, function\n",
      "Top words for topic 14: model, set, comput, l, r, train, use, time, approxim, paramet\n",
      "Top words for topic 15: j, time, p, network, data, weight, f, estim, model, approxim\n",
      "Top words for topic 16: c, function, system, differ, data, problem, valu, k, exampl, b\n",
      "Top words for topic 17: model, set, function, one, c, system, r, state, distribut, process\n",
      "Top words for topic 18: network, learn, differ, figur, set, problem, p, input, result, comput\n",
      "Top words for topic 19: e, train, c, valu, system, exampl, l, r, function, x\n"
     ]
    }
   ],
   "source": [
    "for t in range(topics_count):\n",
    "    top_words = np.argsort(phi[:, t])[::-1][:10]\n",
    "    top_words = [id2word[i] for i in top_words]\n",
    "    print(f\"Top words for topic {t}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588dc900d7ff711b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
